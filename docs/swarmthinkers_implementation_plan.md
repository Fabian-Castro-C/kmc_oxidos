# SwarmThinkers Implementation Plan for kmc_oxidos

**Branch**: `feature/swarmthinkers-integration`  
**Objetivo**: Integrar framework SwarmThinkers para acelerar simulaciones KMC de deposiciÃ³n TiOâ‚‚

---

## ğŸ“‹ Resumen Ejecutivo

SwarmThinkers es un framework de RL que trata a cada Ã¡tomo/sitio como un agente que propone transiciones localmente. Una polÃ­tica centralizada aprende a priorizar eventos estructuralmente importantes, acelerando la simulaciÃ³n sin perder fidelidad fÃ­sica mediante importance sampling.

### Diferencias Clave vs Paper Original

| Aspecto | Paper (Fe-Cu difusiÃ³n) | kmc_oxidos (TiOâ‚‚ deposiciÃ³n) |
|---------|------------------------|------------------------------|
| Proceso | DifusiÃ³n de vacancias (cerrado) | DeposiciÃ³n + crecimiento (abierto) |
| Eventos | Solo DIFFUSION | ADSORPTION, DIFFUSION, REACTION, DESORPTION |
| Especies | 1 (vacancias) | 3 (Ti, O, VACANT) |
| Reacciones | âŒ No | âœ… Ti + 2O â†’ TiOâ‚‚ |
| Barrera ES | âŒ No | âœ… DifusiÃ³n descendente |

---

## ğŸ¯ Fases de ImplementaciÃ³n

### **Fase 1: Prototipo Diffusion-Only** â¬…ï¸ EMPEZAMOS AQUÃ

**Objetivo**: Validar mecÃ¡nica SwarmThinkers bÃ¡sica sin complejidad de mÃºltiples eventos.

**Scope**:
- Solo eventos de difusiÃ³n (DIFFUSION_TI, DIFFUSION_O)
- PolÃ­tica simple: propone K direcciones de difusiÃ³n por Ã¡tomo adsorbido
- ValidaciÃ³n: comparar distribuciones vs KMC clÃ¡sico en lattice pequeÃ±o

**Componentes a Implementar**:

1. **Observaciones Locales** (`src/rl/observations.py`):
   ```python
   def get_local_observation(lattice, site_idx) -> np.ndarray:
       """
       Returns:
           - [0:36]: especies 1st neighbors (one-hot: VACANT, TI, O)
           - [36:48]: alturas relativas (z_neighbor - z_site) para ES barrier
           - [48:50]: composiciÃ³n local (n_Ti, n_O)
           - [50]: altura z absoluta
       """
   ```

2. **SwarmPolicy Simple** (`src/rl/swarm_policy.py`):
   ```python
   class DiffusionSwarmPolicy(nn.Module):
       """Policy que solo propone direcciones de difusiÃ³n."""
       def forward(self, obs) -> torch.Tensor:
           # Returns: logits para K direcciones (12 vecinos)
   ```

3. **SwarmEngine BÃ¡sico** (`src/rl/swarm_engine.py`):
   ```python
   class SwarmEngine:
       def generate_diffusion_proposals(policy, lattice, n_swarm):
           # 1. Get adsorbed atoms
           # 2. Policy propone direcciones
           # 3. Calcula tasas con RateCalculator (incluye ES barrier)
           # 4. Reweighting: P(a) = Ï€(a)Â·Î“_a / Z
           # 5. Select + importance weight
   ```

4. **Experimento ValidaciÃ³n** (`experiments/validate_swarmthinkers_phase1.py`):
   - Lattice 20Ã—20Ã—10, temperatura 180K
   - Ejecutar 10k steps con KMC clÃ¡sico
   - Ejecutar 10k steps con SwarmThinkers
   - Comparar: roughness, coverage, distribuciÃ³n de especies
   - Test estadÃ­stico: Kolmogorov-Smirnov para unbiasedness

**Criterios de Ã‰xito Fase 1**:
- âœ… Importance weights convergen (ESS > 0.5)
- âœ… Distribuciones finales indistinguibles (p-value > 0.05 en KS test)
- âœ… SwarmThinkers completa simulaciÃ³n sin crashes
- âœ… CÃ³digo documentado y testeado

---

### **Fase 2: Eventos Completos**

**Objetivo**: Extender a todos los tipos de eventos del sistema.

**Scope**:
- Agregar ADSORPTION_TI, ADSORPTION_O, DESORPTION_TI, DESORPTION_O, REACTION_TIO2
- Policy con mÃºltiples heads (uno por tipo de evento)
- Action masking robusto

**Componentes Nuevos**:

1. **MultiEventPolicy** (`src/rl/swarm_policy.py`):
   ```python
   class TiO2SwarmPolicy(nn.Module):
       """Policy con heads especializados por tipo de evento."""
       - head_diffusion: K direcciones
       - head_adsorption: 2 especies (Ti, O)
       - head_desorption: 1 probabilidad
       - head_reaction: 1 probabilidad
   ```

2. **Action Masking** (`src/rl/action_masking.py`):
   ```python
   def get_valid_actions(agent_idx, lattice) -> Dict[ActionType, bool]:
       # VACANT_SURFACE -> solo ADSORPTION
       # TI_ADSORBED -> DIFFUSION + DESORPTION + REACTION (si 2+ O vecinos)
       # O_ADSORBED -> DIFFUSION + DESORPTION
   ```

3. **SwarmEngine Completo** (`src/rl/swarm_engine.py`):
   - Dispatch de tasas segÃºn tipo de evento
   - Soporte para reacciones multi-site
   - Global softmax sobre todos (agente, acciÃ³n) pairs

**ValidaciÃ³n Fase 2**:
- Comparar formaciÃ³n de TiOâ‚‚ con KMC clÃ¡sico
- Verificar que reacciones ocurren en configuraciones correctas
- Measure effective transition ratio (ETR)

---

### **Fase 3: Training con RL**

**Objetivo**: Entrenar polÃ­tica para maximizar eficiencia manteniendo fÃ­sica.

**Scope**:
- Setup entrenamiento PPO
- Recompensa: `r_t = -Î”E_t` (minimizar energÃ­a)
- Critic centralizado con estadÃ­sticas globales
- Generalization: entrenar en 10Ã—10Ã—10, evaluar en 40Ã—40Ã—20

**Componentes**:

1. **SwarmEnvironment** (`src/rl/swarm_environment.py`):
   ```python
   class TiO2SwarmEnv(gym.Env):
       """Gymnasium env con SwarmEngine en el loop."""
       - Observations: local obs para cada agente activo
       - Actions: selecciÃ³n de evento vÃ­a swarm
       - Rewards: -Î”E por step
       - Info: importance weights, ESS
   ```

2. **Training Script** (`experiments/train_swarm_policy.py`):
   - PPO con Stable-Baselines3
   - Entropy regularization para exploraciÃ³n
   - Checkpoints cada 10k steps
   - Tensorboard logging

3. **MÃ©tricas de Performance**:
   - Speedup ratio: steps_KMC / steps_swarm para igual evoluciÃ³n
   - Effective transition ratio (ETR): eventos productivos / total
   - Memory usage
   - Walltime per step

**Objetivos de Performance**:
- ğŸ¯ Speedup > 10Ã— en lattices grandes (>40Ã—40Ã—40)
- ğŸ¯ ETR > 0.1 (vs < 0.001 en KMC clÃ¡sico)
- ğŸ¯ Memory < 2 GB para 50Ã—50Ã—30 lattice

---

## ğŸ—ï¸ Arquitectura de CÃ³digo

### Estructura de Archivos Nueva

```
src/rl/
â”œâ”€â”€ __init__.py              # Actualizar exports
â”œâ”€â”€ observations.py          # ğŸ†• Local observation extraction
â”œâ”€â”€ swarm_policy.py          # ğŸ†• DiffusionSwarmPolicy + TiO2SwarmPolicy
â”œâ”€â”€ swarm_engine.py          # ğŸ†• SwarmEngine core logic
â”œâ”€â”€ action_masking.py        # ğŸ†• Valid actions per agent type (Fase 2)
â”œâ”€â”€ swarm_environment.py     # ğŸ†• Gymnasium env (Fase 3)
â”œâ”€â”€ policy.py                # âœ… Mantener para baseline
â”œâ”€â”€ critic.py                # âœ… Mantener, usar en Fase 3
â”œâ”€â”€ reweighting.py           # âœ… Ya existe, reutilizar
â””â”€â”€ environment.py           # âœ… Mantener para comparaciÃ³n

experiments/
â”œâ”€â”€ validate_swarmthinkers_phase1.py  # ğŸ†• ValidaciÃ³n diffusion-only
â”œâ”€â”€ validate_swarmthinkers_phase2.py  # ğŸ†• ValidaciÃ³n multi-evento
â”œâ”€â”€ train_swarm_policy.py             # ğŸ†• Training PPO (Fase 3)
â””â”€â”€ compare_swarm_vs_classic.py       # ğŸ†• Benchmarks completos
```

### Principios de DiseÃ±o

1. **No modificar `src/kmc/`**: MÃ³dulo KMC permanece puro y clÃ¡sico
2. **Composition over Inheritance**: SwarmEngine compone KMCSimulator, no hereda
3. **SeparaciÃ³n de concerns**:
   - `swarm_engine.py`: LÃ³gica de swarm (propuestas, reweighting, selection)
   - `swarm_policy.py`: Redes neuronales
   - `observations.py`: Feature engineering
   - `action_masking.py`: ValidaciÃ³n de acciones
4. **Testabilidad**: Cada componente con unit tests
5. **Reproducibilidad**: Seeds fijos en experimentos de validaciÃ³n

---

## ğŸ“Š ValidaciÃ³n y MÃ©tricas

### Correctness (Physics Fidelity)

**Test de Unbiasedness**:
```python
# Ejecutar N trials con ambos mÃ©todos
roughness_classic = [run_kmc_classic() for _ in range(50)]
roughness_swarm = [run_swarm() for _ in range(50)]

# Kolmogorov-Smirnov test
ks_statistic, p_value = ks_2samp(roughness_classic, roughness_swarm)
assert p_value > 0.05, "Distributions differ significantly"
```

**MÃ©tricas FÃ­sicas**:
- Roughness evolution W(t)
- Coverage Î¸(t)
- ComposiciÃ³n (Ti/O ratio)
- FormaciÃ³n TiOâ‚‚ (# molÃ©culas vs tiempo)
- Exponentes de scaling Î±, Î²

### Performance

**Speedup Ratio**:
```
SR = steps_classic_needed / steps_swarm_needed
```
donde ambos alcanzan misma configuraciÃ³n morfolÃ³gica.

**Effective Sample Size (ESS)**:
```
ESS = (Î£ w_i)Â² / Î£ w_iÂ²
```
donde w_i son importance weights. ESS > 0.5 indica buen sampling.

**Effective Transition Ratio (ETR)**:
```
ETR = eventos_productivos / total_eventos
```
donde evento productivo = causa cambio estructural (no reversible inmediato).

---

## ğŸš€ EjecuciÃ³n Fase 1

### Checklist de ImplementaciÃ³n

- [ ] Crear `src/rl/observations.py` con `get_local_observation()`
- [ ] Crear `src/rl/swarm_policy.py` con `DiffusionSwarmPolicy`
- [ ] Crear `src/rl/swarm_engine.py` con lÃ³gica bÃ¡sica de swarm
- [ ] Crear `experiments/validate_swarmthinkers_phase1.py`
- [ ] Ejecutar validaciÃ³n en lattice 20Ã—20Ã—10
- [ ] Analizar resultados: KS test, ESS, visual comparison
- [ ] Documentar findings en `docs/swarmthinkers_phase1_results.md`
- [ ] Commit y push a branch
- [ ] Si todo OK â†’ merge a master y pasar a Fase 2

### Comandos de EjecuciÃ³n

```powershell
# ValidaciÃ³n Fase 1
uv run python -m experiments.validate_swarmthinkers_phase1 `
    --lattice-size 20 20 10 `
    --temperature 180 `
    --max-steps 10000 `
    --n-trials 50 `
    --swarm-size 32 `
    --seed 42

# Genera outputs:
# - results/swarmthinkers_phase1/comparison_plots.png
# - results/swarmthinkers_phase1/ks_test_results.json
# - results/swarmthinkers_phase1/importance_weights_evolution.png
```

---

## ğŸ“š Referencias

- **Paper Original**: SwarmThinkers: Learning Physically Consistent Atomic KMC Transitions at Scale (Li et al., 2025)
- **CÃ³digo Base**: `src/kmc/` (KMC clÃ¡sico ya implementado)
- **RL Framework**: `src/rl/` (ActorNetwork, CriticNetwork, ReweightingMechanism ya existen)
- **DocumentaciÃ³n Proyecto**: `docs/FRAMEWORK_SUMMARY.md`, `docs/agent/GUIDEMENT.md`

---

## âœ… Criterios de Ã‰xito General

**Fase 1 (Prototipo)**:
- âœ… No crashes durante simulaciÃ³n
- âœ… Importance weights estables (ESS > 0.5)
- âœ… Distribuciones fÃ­sicas correctas (KS p-value > 0.05)

**Fase 2 (Multi-Evento)**:
- âœ… Todos los tipos de eventos funcionan
- âœ… Action masking correcto (no propuestas invÃ¡lidas)
- âœ… Reacciones TiOâ‚‚ ocurren en configuraciones esperadas

**Fase 3 (Training)**:
- âœ… Policy aprende (reward aumenta durante training)
- âœ… Speedup > 10Ã— vs KMC clÃ¡sico
- âœ… Generaliza a lattices mÃ¡s grandes sin reentrenar

---

**Notas**:
- Este plan es iterativo: ajustaremos segÃºn resultados de cada fase
- Prioridad = correctness > speedup
- Documentar decisiones de diseÃ±o en commits
