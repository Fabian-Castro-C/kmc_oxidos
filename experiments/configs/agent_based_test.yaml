# Agent-based SwarmThinkers training configuration
# Smaller scale for faithful paper implementation

# Environment configuration
environment:
  lattice_size: [5, 5, 5]      # Small lattice for testing
  temperature_range: [300.0, 900.0]  # Temperature range for generalization
  deposition_rate: 1.0
  max_steps: 50                 # Short episodes
  max_agents: 64                # Maximum agents (padded)
  use_reweighting: true         # Enable physical rate reweighting
  # NOTE: reward_weights not used - using SwarmThinkers paper reward: r_t = -ΔE
  # See src/training/energy_calculator.py for implementation

# PPO hyperparameters (adjusted for agent-based)
ppo:
  learning_rate: 3.0e-4
  n_steps: 1024                 # Smaller buffer (fewer transitions per agent)
  batch_size: 32                # Smaller batches
  n_epochs: 10
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  ent_coef: 0.02                # Higher exploration
  vf_coef: 0.5
  max_grad_norm: 0.5

# Training configuration
total_timesteps: 50_000         # Entrenamiento corto para prueba rápida
n_envs: 4                       # Fewer parallel envs (agent-based is slower)

# Checkpoint configuration
checkpoint_freq: 10_000
eval_freq: 5_000
eval_episodes: 3

# Agent-based specific
use_dict_obs: true              # Dict observation space
mask_invalid_actions: true      # Mask padding actions
