# Agent-based SwarmThinkers training configuration
# Smaller scale for faithful paper implementation

# Environment configuration
environment:
  lattice_size: [5, 5, 5]      # Small lattice for testing
  temperature: 600.0
  deposition_rate: 1.0
  max_steps: 50                 # Short episodes
  max_agents: 64                # Maximum agents (padded)
  use_reweighting: true         # Enable physical rate reweighting
  reward_weights:
    roughness_weight: -1.0
    coverage_weight: 0.5
    stoichiometry_weight: -0.3  # Ti:O ratio penalty
    ess_weight: -0.1             # ESS penalty

# PPO hyperparameters (adjusted for agent-based)
ppo:
  learning_rate: 3.0e-4
  n_steps: 1024                 # Smaller buffer (fewer transitions per agent)
  batch_size: 32                # Smaller batches
  n_epochs: 10
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  ent_coef: 0.02                # Higher exploration
  vf_coef: 0.5
  max_grad_norm: 0.5

# Training configuration
total_timesteps: 50_000         # 50k steps for initial testing
n_envs: 4                       # Fewer parallel envs (agent-based is slower)

# Checkpoint configuration
checkpoint_freq: 10_000
eval_freq: 5_000
eval_episodes: 3

# Agent-based specific
use_dict_obs: true              # Dict observation space
mask_invalid_actions: true      # Mask padding actions
